{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error-Correcting Output Coding\n",
    "\n",
    "\n",
    "#### David Solans \n",
    "#### Huang Chen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extends the notebook posted by David Solans in his github: \n",
    " https://github.com/dsolanno/UBDataScience-Machine-Learning/blob/master/Error%20Correcting%20Output%20Codes%20(ECOC)/Error%20Correcting%20Output%20Coding.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Introduction\n",
    "Error-Correcting Output Codes(ECOC) is an ensemble method designed for\n",
    "multi-class classification problem. In multi-class classification problem, the task\n",
    "is to decide one label from $k > 2$ possible choices. \n",
    "\n",
    "For example, in digit recognition task, we need to map each hand written digit to one of $k = 10$ classes.\n",
    "Some algorithms, such as decision tree, naive bayes and neural network, can\n",
    "handle multi-class problem directly.\n",
    "ECOC is a meta method which combines many binary classifiers in order to\n",
    "solve the multi-class problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](ecoc1.png)\n",
    "<p style=\"text-align: center;\">Figure 1: A 15 bit error-correcting output code for a ten-class problem</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 shows a 15 bit error-correcting output code for a ten-class problem.\n",
    "\n",
    "Each class is assigned a unique binary string of length 15. The string is also\n",
    "called a codeword. For example, class 2 has the codeword $100100011110101$.\n",
    "During training, one binary classifier is learned for each column. For example,\n",
    "for the first column, we build a binary classifier to separate ${0, 2, 4, 6, 8}$ from\n",
    "${1, 3, 5, 7, 9}$. Thus 15 binary classifiers are trained in this way. To classify a\n",
    "new data point x, all 15 binary classifiers are evaluated to obtain a 15-bit string.\n",
    "Finally, we choose the class whose codeword is closet to x’s output string as the\n",
    "predicted label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Theoretical Justification\n",
    "Notice that in the error-correcting output code, the rows have more bits than\n",
    "is necessary. $log_2 10 = 4$ is enough for representing 10 different classes. Using\n",
    "some redundant ”error-correcting” bits, we can tolerant some error introduced\n",
    "by finite training sample, poor choice of input features, and flaws in the training\n",
    "algorithm. Thus the system is more likely to recover from the errors. If the\n",
    "minimum Hamming distance between any pair of code words is d, then the code\n",
    "can correct at least $|\\frac{d−1}{2}|$ single bit errors. As long as the error moves us fewer\n",
    "than $|\\frac{d−1}{2}|$ unit away from the true codeword, the nearest codeword is still the\n",
    "correct one. The code in Figure 1 can correct up to 3 errors out of the 15 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Code design\n",
    "There are many ways to design the error-correcting output code. We can list the following 4 different method for constructing good error-correcting output codes\n",
    "\n",
    "1. Exhaustive codes. It is applied when $3\\leq k <7$\n",
    "2. Column Selection from exhaustive codes for $8 \\leq k \\leq 11$\n",
    "3. Randomized hill climbing and Bose-Chaudhuri-Hocquengham(BCH) codes when $k>11$.\n",
    "\n",
    "Besides, random generation of the codewords is a recommended method by the reference [http://www.jair.org/vol/vol2.html](\"Solving  multiclass  learning  problem  via  error-correcting  output codes,\" Journal of Artificial Intelligence Research, vol. 2, pp. 263–286, 1995.)\n",
    "\n",
    "\n",
    "Here we introduce each of the above methods. But we give a simple example to focus on exhaustive codes.\n",
    "\n",
    "**1.Exhaustive Codes **\n",
    "\n",
    "When the number of classes k is small $(3 < k ≤ 7)$, we can use exhaustive\n",
    "codes. Each code has length $2^{k−1}−1$. \n",
    "\n",
    "- Row 1 contains only ones. \n",
    "\n",
    "- Row 2 consists\n",
    "of $2^{k−2}$ zeros followed by $2^{k−2} − 1$ ones.\n",
    "\n",
    "- Row 3 consists of $2^{k−3}$ zeros, followed\n",
    "by $2^{k−3}$ ones, followed by $2^{k−3}$\n",
    "zeros, followed by $2^{k−3} −1$ ones. \n",
    "\n",
    "Figure 2 shows\n",
    "the exhaustive code for a five-class problem. The code has inter-row Hamming\n",
    "distance 8.\n",
    "When the number of classes k is large, random codes can be used. The major benefit of error-corrective coding is variance reduction via model averaging. Random code works as well as optimally\n",
    "constructed code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](ecoc2.png)\n",
    "<p style=\"text-align: center;\">Figure 2: Exhaustive code for a five class problem</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Column Selection from exhaustive codes for **\n",
    "\n",
    "When $8\\leq k\\leq 11$, we construct an exhaustive code and then select a good subset of  its columns.\n",
    "We formulate this as a propositional satisability problem and apply the GSAT algorithm (Selman,Levesque, &Mitchell,1992) to attempt a solution. A solution is required to include exactly $L$ columns(the desired length of the code) while ensuring that the Hamming distance between every two columns is between $d$ and $L$ exclusion constraint is placed between any two columns that violate the column separation condition. To support these constraints, we\n",
    "extended GSAT to support mutual exclusion and \"m-of-n\" constraints efficiently.\n",
    "\n",
    "**3. Randomized hill climbing and Bose-Chaudhuri-Hocquengham(BCH) codes **\n",
    "\n",
    "**Randomized hill climbing**\n",
    "\n",
    "For $k>11$, we employed a random search algorithm that begins by drawing $k$ random strings of the desired length $L$. Any pair of such random strings will be separated by a Hamming distance that is binomially distributed with mean $L=2$.\n",
    "Hence, such randomly generated codes are generally quite good on average. To improve them,the algorithm repeatedly finds the pair of rows closest together in Hamming distance and the pair of columns that have the \"most extreme\" Hamming distance (i.e.,either too close or too far apart). The algorithm then computes the four codeword bits where these rows and columns intersect and changes them to improve the row and column separations as shown in the below figure. When this hill-climbing procedure reaches a local maximum, the algorithm randomly chooses pairs of rows and columns and tries to improve their separations. This combined hill-climbing/random-choice procedure is able to improve the minimum Hamming distance separation quite substantially.\n",
    "\n",
    "![alt text](random_hill_climbing.jpg)\n",
    "<p style=\"text-align: center;\">Figure 3: random_hill_climbing for\n",
    "improving row and column separation. The two closest rows and columns are indicated by lines.Where these lines intersect, the bits in the codewords are changed to improve separations as shown on the right.</p>\n",
    "\n",
    "**Bose-Chaudhuri-Hocquengham(BCH)**\n",
    "\n",
    "For $k>11$ we can also applied the BCH algorithm to design codes (Bose & Ray-Chaudhuri, 1960;Hocquenghem,1959). The BCH algorithm employs algebraic methods from Galois field theory to design nearly optimal error-correcting codes. However, there are three practical drawbacks to using this algorithm. First, published tables of the primitive polynomials required by this algorithm only produce codes up to length 64, since this is the largest word size employed in computer\n",
    "memories. Second, the codes do not always exhibit good column separations. Third, the number of rows in these codes is\n",
    "always a power of two. If the number of classes $k$ in our learning problem is not a power of two, we must shorten the code by deleting rows (and possible columns) while maintaining good row and column separations. \n",
    "\n",
    "\n",
    "**We can find more details about the about method, also some real experiment in the following article:[https://www.jair.org/media/105/live-105-1426-jair.pdf](The above descriptions come from this article)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Working with ECOC in python\n",
    "\n",
    "We will use the class:\n",
    "<br>\n",
    "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OutputCodeClassifier.html\"> sklearn.multiclass.OutputCodeClassifier</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OutputCodeClassifier, the `code_size` attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes.\n",
    "\n",
    "A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. In theory, `log2(n_classes) / n_classes` is sufficient to represent each class unambiguously. However, in practice, it may not lead to good accuracy since `log2(n_classes)` is much smaller than n_classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.92666666666666664"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf = OutputCodeClassifier(LinearSVC(random_state=0),\n",
    "                           code_size=2, random_state=0)\n",
    "yhat=clf.fit(X, y).predict(X)\n",
    "print(yhat)\n",
    "accuracy_score(yhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf = OutputCodeClassifier(LinearSVC(random_state=0), code_size=15, random_state=0)\n",
    "yhat=clf.fit(X, y).predict(X)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97999999999999998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1],\n",
       "       [0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1],\n",
       "       [0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codewords=np.array([[0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0 ,0 ,0 ,0, 0],[0, 0, 1, 0, 0, 0, 1 ,0, 0 ,0 ,1 ,1, 0 ,0, 1],[0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1],[1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1],[0 ,1 ,1 ,0 ,1 ,1 ,0 ,1 ,1 ,0 ,0 ,0 ,0 ,0, 1],[1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0],[1 ,1 ,1 ,0 ,0 ,1 ,1 ,0 ,1 ,0 ,0 ,1 ,1 ,0, 1],[1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0]])\n",
    "codewords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46666666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "HL=[distance.hamming(codewords[0],codewords[1]) for i in ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns=[\"L\"+str(i+1) for i in range(codewords.shape[1])]\n",
    "index=[\"K\"+str(i+1) for i in range(codewords.shape[0])]\n",
    "df = pd.DataFrame(codewords,columns=columns,index=index)\n",
    "def convert_str(df):\n",
    "    x=\"\"\n",
    "    for i in df:\n",
    "        x+=str(i)\n",
    "    return x\n",
    "df['string']=df.apply(lambda x: convert_str(x),axis=1)\n",
    "comparison_list=df['string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming2(s1, s2):\n",
    "    \"\"\"Calculate the Hamming distance between two bit strings\"\"\"\n",
    "    assert len(s1) == len(s2)\n",
    "    return sum(c1 != c2 for c1, c2 in zip(s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 7\n",
      "0 4 5\n",
      "3 5 4\n"
     ]
    }
   ],
   "source": [
    "min_k_haming=1000\n",
    "for i in range(len(comparison_list)):\n",
    "    for j in range(i+1,len(comparison_list)):\n",
    "        if(hamming2(comparison_list[i], comparison_list[j])<min_k_haming):   \n",
    "            min_k_haming=hamming2(comparison_list[i], comparison_list[j])\n",
    "            print(i,j,min_k_haming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2\n",
      "1 5 0\n"
     ]
    }
   ],
   "source": [
    "df_L = pd.DataFrame(codewords.T,columns=index,index=columns)\n",
    "df_L['string']=df_L.apply(lambda x: convert_str(x),axis=1)\n",
    "comparison_list_L=df_L['string']\n",
    "min_L_haming=1000\n",
    "for i in range(len(comparison_list_L)):\n",
    "    for j in range(i+1,len(comparison_list_L)):\n",
    "        if(hamming2(comparison_list_L[i], comparison_list_L[j])<min_L_haming):   \n",
    "            min_L_haming=hamming2(comparison_list_L[i], comparison_list_L[j])\n",
    "            print(i,j,min_L_haming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "0.466666666667\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "0.333333333333\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "0.266666666667\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[0 0 1 1 1 0 0 1 0 0 0 0 0 0 0] 0\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[0 0 1 0 0 0 1 0 0 0 1 1 0 0 1] 1\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 0 1] 2\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[1 1 1 1 0 1 1 1 1 0 1 1 0 0 1] 3\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[0 1 1 0 1 1 0 1 1 0 0 0 0 0 1] 4\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1 0 1 0] 5\n",
      "[1 1 0 0 1 1 0 1 0 0 1 0 0 1 0] 7\n",
      "[1 1 1 0 0 1 1 0 1 0 0 1 1 0 1] 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_list=np.linspace(0, codewords.shape[0]-1, num=codewords.shape[0]).astype(int)\n",
    "min_k_haming=1000\n",
    "for row in range(codewords.shape[0]):\n",
    "    for compare_row in np.delete(compare_list,row):\n",
    "        print(codewords[row],row)\n",
    "        print(codewords[compare_row],compare_row)\n",
    "        if distance.hamming(codewords[row],codewords[compare_row])<min_k_haming:\n",
    "            print(distance.hamming(codewords[row],codewords[compare_row]))\n",
    "            min_k_haming=distance.hamming(codewords[row],codewords[compare_row])\n",
    "min_k_haming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "0.25\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "0.0\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 0 0 0 0 1 0 1] 13\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 0 1 0 1 1 1] 0\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 1 1 1 1 1 1] 1\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[1 1 1 1 1 1 1 0] 2\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[1 0 0 1 0 0 0 0] 3\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[1 0 0 0 1 0 0 1] 4\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 1 1 1 1 1 1] 5\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 1 0 1 0 0 1 0] 6\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[1 0 0 1 1 1 0 1] 7\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 0 1 1 1 1 0] 8\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 1 0 0 0 0 0] 9\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 1 1 1 0 1 0 1] 10\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 1 0 1 0 1 1 0] 11\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 0 0 0 0 1 0] 12\n",
      "[0 1 1 1 1 0 1 0] 14\n",
      "[0 0 0 0 0 1 0 1] 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_list=np.linspace(0, codewords.T.shape[0]-1, num=codewords.T.shape[0]).astype(int)\n",
    "min_k_haming=1000\n",
    "for row in range(codewords.T.shape[0]):\n",
    "    for compare_row in np.delete(compare_list,row):\n",
    "        print(codewords.T[row],row)\n",
    "        print(codewords.T[compare_row],compare_row)\n",
    "        if distance.hamming(codewords.T[row],codewords.T[compare_row])<min_k_haming:\n",
    "            print(distance.hamming(codewords.T[row],codewords.T[compare_row]))\n",
    "            min_k_haming=distance.hamming(codewords.T[row],codewords.T[compare_row])\n",
    "min_k_haming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_list"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
